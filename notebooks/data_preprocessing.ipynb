{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df37ec0a-eb0a-442f-8cd0-25f6689ae564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ekaterina_Dul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ekaterina_Dul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Ekaterina_Dul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Ekaterina_Dul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "from multiprocessing import Pool\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f08ec3ff-67db-4cc9-b474-0ab504cc839d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I caught this little gem totally by accident b...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I can't believe that I let myself into this mo...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*spoiler alert!* it just gets to me the nerve ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If there's one thing I've learnt from watching...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I remember when this was in theaters, reviews ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  I caught this little gem totally by accident b...  positive\n",
       "1  I can't believe that I let myself into this mo...  negative\n",
       "2  *spoiler alert!* it just gets to me the nerve ...  negative\n",
       "3  If there's one thing I've learnt from watching...  negative\n",
       "4  I remember when this was in theaters, reviews ...  negative"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df = pd.read_csv(\"../data/raw/final_project_train_dataset/train.csv\", sep=',')\n",
    "reviews_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbae39ad-f30e-4f0a-8e11-42ce4f267881",
   "metadata": {},
   "source": [
    "# Dataset preprocessing.\n",
    "\n",
    "In the following section we will focus on general train dataset cleansing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5420ce-e763-4839-9d73-d9905b54d280",
   "metadata": {},
   "source": [
    "## Main features extraction.\n",
    "\n",
    "Extraction of significant numerical features from `review` column, based on EDA summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "007b7df4-1816-41b3-8dc3-fca84d336ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df['number_of_chars'] = reviews_df['review'].apply(len)\n",
    "reviews_df['percentage_of_signs'] = reviews_df['review'].apply(lambda x: sum([1 for c in x if not c.isalpha()]) / len(x) * 100)\n",
    "reviews_df['number_of_excl_marks'] = reviews_df['review'].apply(lambda x: x.count('!'))\n",
    "reviews_df['number_of_question_marks'] = reviews_df['review'].apply(lambda x: x.count('?'))\n",
    "reviews_df['number_of_ellipses'] = reviews_df['review'].apply(lambda x: x.count('...'))\n",
    "reviews_df['number_of_uppercase_words'] = reviews_df['review'].apply(lambda x: sum([1 for w in x.split() if re.sub(r'[^a-zA-Z]', '', w).isupper()]))\n",
    "\n",
    "numerical_review_features = [\n",
    "    'number_of_chars',\n",
    "    'percentage_of_signs',\n",
    "    'number_of_excl_marks',\n",
    "    'number_of_question_marks',\n",
    "    'number_of_ellipses',\n",
    "    'number_of_uppercase_words'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe9c0b6-06d0-43cf-a3d2-5fb23ace14b1",
   "metadata": {},
   "source": [
    "## Duplicates removal.\n",
    "\n",
    "Removal of duplicated rows. As was discussed in EDA, in train dataset we don't have similar reviews with different sentiments, therefore no additional quality check is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd6526a0-9996-4fb0-8255-e6c62b47fd5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd3e7220-5bd1-49e8-b79c-247c4a964c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39728"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.drop_duplicates(inplace=True)\n",
    "len(reviews_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a9b9d7-46bc-4bbb-9683-844a0eb3b877",
   "metadata": {},
   "source": [
    "## Outliers removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33cf85a8-eca9-4e91-9ed9-96aabe3d74e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>number_of_chars</th>\n",
       "      <td>39728.0</td>\n",
       "      <td>1311.359469</td>\n",
       "      <td>988.798970</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>699.000000</td>\n",
       "      <td>971.500000</td>\n",
       "      <td>1596.000000</td>\n",
       "      <td>13704.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percentage_of_signs</th>\n",
       "      <td>39728.0</td>\n",
       "      <td>21.976804</td>\n",
       "      <td>1.827637</td>\n",
       "      <td>11.764706</td>\n",
       "      <td>20.802836</td>\n",
       "      <td>21.829396</td>\n",
       "      <td>22.939068</td>\n",
       "      <td>87.311178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_excl_marks</th>\n",
       "      <td>39728.0</td>\n",
       "      <td>0.972563</td>\n",
       "      <td>2.964011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>282.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_question_marks</th>\n",
       "      <td>39728.0</td>\n",
       "      <td>0.646018</td>\n",
       "      <td>1.497642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_ellipses</th>\n",
       "      <td>39728.0</td>\n",
       "      <td>0.499522</td>\n",
       "      <td>1.583290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_uppercase_words</th>\n",
       "      <td>39728.0</td>\n",
       "      <td>4.877014</td>\n",
       "      <td>5.592917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>151.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             count         mean         std        min  \\\n",
       "number_of_chars            39728.0  1311.359469  988.798970  41.000000   \n",
       "percentage_of_signs        39728.0    21.976804    1.827637  11.764706   \n",
       "number_of_excl_marks       39728.0     0.972563    2.964011   0.000000   \n",
       "number_of_question_marks   39728.0     0.646018    1.497642   0.000000   \n",
       "number_of_ellipses         39728.0     0.499522    1.583290   0.000000   \n",
       "number_of_uppercase_words  39728.0     4.877014    5.592917   0.000000   \n",
       "\n",
       "                                  25%         50%          75%           max  \n",
       "number_of_chars            699.000000  971.500000  1596.000000  13704.000000  \n",
       "percentage_of_signs         20.802836   21.829396    22.939068     87.311178  \n",
       "number_of_excl_marks         0.000000    0.000000     1.000000    282.000000  \n",
       "number_of_question_marks     0.000000    0.000000     1.000000     35.000000  \n",
       "number_of_ellipses           0.000000    0.000000     0.000000     48.000000  \n",
       "number_of_uppercase_words    1.000000    3.000000     6.000000    151.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de004f2a-da44-4a00-b6b8-129cd73b4272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate IQR for column 'number_of_chars'\n",
    "Q1 = reviews_df['number_of_chars'].quantile(0.25)\n",
    "Q3 = reviews_df['number_of_chars'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# identify outliers\n",
    "threshold = 1.5\n",
    "outliers = reviews_df[(reviews_df['number_of_chars'] < Q1 - threshold * IQR) | (reviews_df['number_of_chars'] > Q3 + threshold * IQR)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "270e50b0-e219-4763-b0c6-82f76881ad25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2958"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "613edaa8-06d2-4481-a7ba-4834bcf334e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36770"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.drop(outliers.index, inplace=True)\n",
    "len(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8108acff-c064-4c7b-9302-d68f5cc9998a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>number_of_chars</th>\n",
       "      <td>36770.0</td>\n",
       "      <td>1094.826435</td>\n",
       "      <td>595.737973</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>685.000000</td>\n",
       "      <td>918.000000</td>\n",
       "      <td>1393.000000</td>\n",
       "      <td>2941.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percentage_of_signs</th>\n",
       "      <td>36770.0</td>\n",
       "      <td>22.011077</td>\n",
       "      <td>1.863173</td>\n",
       "      <td>11.764706</td>\n",
       "      <td>20.813033</td>\n",
       "      <td>21.868365</td>\n",
       "      <td>23.006231</td>\n",
       "      <td>87.311178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_excl_marks</th>\n",
       "      <td>36770.0</td>\n",
       "      <td>0.909872</td>\n",
       "      <td>2.880267</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>282.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_question_marks</th>\n",
       "      <td>36770.0</td>\n",
       "      <td>0.548545</td>\n",
       "      <td>1.281312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_ellipses</th>\n",
       "      <td>36770.0</td>\n",
       "      <td>0.463204</td>\n",
       "      <td>1.469358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_uppercase_words</th>\n",
       "      <td>36770.0</td>\n",
       "      <td>4.372042</td>\n",
       "      <td>4.524514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>122.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             count         mean         std        min  \\\n",
       "number_of_chars            36770.0  1094.826435  595.737973  41.000000   \n",
       "percentage_of_signs        36770.0    22.011077    1.863173  11.764706   \n",
       "number_of_excl_marks       36770.0     0.909872    2.880267   0.000000   \n",
       "number_of_question_marks   36770.0     0.548545    1.281312   0.000000   \n",
       "number_of_ellipses         36770.0     0.463204    1.469358   0.000000   \n",
       "number_of_uppercase_words  36770.0     4.372042    4.524514   0.000000   \n",
       "\n",
       "                                  25%         50%          75%          max  \n",
       "number_of_chars            685.000000  918.000000  1393.000000  2941.000000  \n",
       "percentage_of_signs         20.813033   21.868365    23.006231    87.311178  \n",
       "number_of_excl_marks         0.000000    0.000000     1.000000   282.000000  \n",
       "number_of_question_marks     0.000000    0.000000     1.000000    25.000000  \n",
       "number_of_ellipses           0.000000    0.000000     0.000000    48.000000  \n",
       "number_of_uppercase_words    1.000000    3.000000     6.000000   122.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd91b810-98a9-410f-b8c4-1785d3f9d46e",
   "metadata": {},
   "source": [
    "# Text preprocessing.\n",
    "\n",
    "Here basic text preprocessing was preformed, and two columns with stemmed tokens and lemmatized tokens were produced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d0e73c-db87-4eac-88ed-5ac10bc6564a",
   "metadata": {},
   "source": [
    "## Punctuation and numbers processing.\n",
    "\n",
    "As it was discussed in EDA, after extracting number of main punctuation marks (ellipses, exclamations, questions), we are ready to remove all punctuation, and not to include it in futher tokenization. Just a reminder, that it was done due to non-standart signs costrustion in initial reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5278ba0-a8c4-462a-ac9f-1a6ecf7b15ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "\n",
    "reviews_df['cleaned_review'] = reviews_df['review'].apply(lambda x: x.replace('<br />', ' ')) \\\n",
    "                                           .apply(lambda x: x.translate(str.maketrans('', '', PUNCT_TO_REMOVE))) \\\n",
    "                                           .apply(lambda x: re.sub(r'[0-9]+', '', x)) \\\n",
    "                                           .apply(lambda x: ''.join(filter(lambda y: y in string.printable, x))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb909a6d-9c61-4500-a852-70524729c941",
   "metadata": {},
   "source": [
    "## Stop-words removal.\n",
    "\n",
    "Removal of all possible standard stop-words variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4388dd3e-4d52-417a-b6c2-3cf57c83cb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A',\n",
       " 'About',\n",
       " 'Above',\n",
       " 'After',\n",
       " 'Again',\n",
       " 'Against',\n",
       " 'Ain',\n",
       " 'All',\n",
       " 'Am',\n",
       " 'An',\n",
       " 'And',\n",
       " 'Any',\n",
       " 'Are',\n",
       " 'Aren',\n",
       " \"Aren'T\",\n",
       " 'ArenT',\n",
       " 'As',\n",
       " 'At',\n",
       " 'Be',\n",
       " 'Because',\n",
       " 'Been',\n",
       " 'Before',\n",
       " 'Being',\n",
       " 'Below',\n",
       " 'Between',\n",
       " 'Both',\n",
       " 'But',\n",
       " 'By',\n",
       " 'Can',\n",
       " 'Couldn',\n",
       " \"Couldn'T\",\n",
       " 'CouldnT',\n",
       " 'D',\n",
       " 'Did',\n",
       " 'Didn',\n",
       " \"Didn'T\",\n",
       " 'DidnT',\n",
       " 'Do',\n",
       " 'Does',\n",
       " 'Doesn',\n",
       " \"Doesn'T\",\n",
       " 'DoesnT',\n",
       " 'Doing',\n",
       " 'Don',\n",
       " \"Don'T\",\n",
       " 'DonT',\n",
       " 'Down',\n",
       " 'During',\n",
       " 'Each',\n",
       " 'Few',\n",
       " 'For',\n",
       " 'From',\n",
       " 'Further',\n",
       " 'Had',\n",
       " 'Hadn',\n",
       " \"Hadn'T\",\n",
       " 'HadnT',\n",
       " 'Has',\n",
       " 'Hasn',\n",
       " \"Hasn'T\",\n",
       " 'HasnT',\n",
       " 'Have',\n",
       " 'Haven',\n",
       " \"Haven'T\",\n",
       " 'HavenT',\n",
       " 'Having',\n",
       " 'He',\n",
       " 'Her',\n",
       " 'Here',\n",
       " 'Hers',\n",
       " 'Herself',\n",
       " 'Him',\n",
       " 'Himself',\n",
       " 'His',\n",
       " 'How',\n",
       " 'I',\n",
       " 'If',\n",
       " 'In',\n",
       " 'Into',\n",
       " 'Is',\n",
       " 'Isn',\n",
       " \"Isn'T\",\n",
       " 'IsnT',\n",
       " 'It',\n",
       " \"It'S\",\n",
       " 'ItS',\n",
       " 'Its',\n",
       " 'Itself',\n",
       " 'Just',\n",
       " 'Ll',\n",
       " 'M',\n",
       " 'Ma',\n",
       " 'Me',\n",
       " 'Mightn',\n",
       " \"Mightn'T\",\n",
       " 'MightnT',\n",
       " 'More',\n",
       " 'Most',\n",
       " 'Mustn',\n",
       " \"Mustn'T\",\n",
       " 'MustnT',\n",
       " 'My',\n",
       " 'Myself',\n",
       " 'Needn',\n",
       " \"Needn'T\",\n",
       " 'NeednT',\n",
       " 'No',\n",
       " 'Nor',\n",
       " 'Not',\n",
       " 'Now',\n",
       " 'O',\n",
       " 'Of',\n",
       " 'Off',\n",
       " 'On',\n",
       " 'Once',\n",
       " 'Only',\n",
       " 'Or',\n",
       " 'Other',\n",
       " 'Our',\n",
       " 'Ours',\n",
       " 'Ourselves',\n",
       " 'Out',\n",
       " 'Over',\n",
       " 'Own',\n",
       " 'Re',\n",
       " 'S',\n",
       " 'Same',\n",
       " 'Shan',\n",
       " \"Shan'T\",\n",
       " 'ShanT',\n",
       " 'She',\n",
       " \"She'S\",\n",
       " 'SheS',\n",
       " 'Should',\n",
       " \"Should'Ve\",\n",
       " 'ShouldVe',\n",
       " 'Shouldn',\n",
       " \"Shouldn'T\",\n",
       " 'ShouldnT',\n",
       " 'So',\n",
       " 'Some',\n",
       " 'Such',\n",
       " 'T',\n",
       " 'Than',\n",
       " 'That',\n",
       " \"That'Ll\",\n",
       " 'ThatLl',\n",
       " 'The',\n",
       " 'Their',\n",
       " 'Theirs',\n",
       " 'Them',\n",
       " 'Themselves',\n",
       " 'Then',\n",
       " 'There',\n",
       " 'These',\n",
       " 'They',\n",
       " 'This',\n",
       " 'Those',\n",
       " 'Through',\n",
       " 'To',\n",
       " 'Too',\n",
       " 'Under',\n",
       " 'Until',\n",
       " 'Up',\n",
       " 'Ve',\n",
       " 'Very',\n",
       " 'Was',\n",
       " 'Wasn',\n",
       " \"Wasn'T\",\n",
       " 'WasnT',\n",
       " 'We',\n",
       " 'Were',\n",
       " 'Weren',\n",
       " \"Weren'T\",\n",
       " 'WerenT',\n",
       " 'What',\n",
       " 'When',\n",
       " 'Where',\n",
       " 'Which',\n",
       " 'While',\n",
       " 'Who',\n",
       " 'Whom',\n",
       " 'Why',\n",
       " 'Will',\n",
       " 'With',\n",
       " 'Won',\n",
       " \"Won'T\",\n",
       " 'WonT',\n",
       " 'Wouldn',\n",
       " \"Wouldn'T\",\n",
       " 'WouldnT',\n",
       " 'Y',\n",
       " 'You',\n",
       " \"You'D\",\n",
       " \"You'Ll\",\n",
       " \"You'Re\",\n",
       " \"You'Ve\",\n",
       " 'YouD',\n",
       " 'YouLl',\n",
       " 'YouRe',\n",
       " 'YouVe',\n",
       " 'Your',\n",
       " 'Yours',\n",
       " 'Yourself',\n",
       " 'Yourselves',\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'arent',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'couldnt',\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'didnt',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doesnt',\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'dont',\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hadnt',\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'hasnt',\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'havent',\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'isnt',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mightnt',\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'mustnt',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'neednt',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shant',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'shes',\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'shouldnt',\n",
       " 'shouldve',\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'thatll',\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'wasnt',\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'werent',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wont',\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'wouldnt',\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'youd',\n",
       " 'youll',\n",
       " 'your',\n",
       " 'youre',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'youve'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "STOPWORDS = STOPWORDS.union(set([w.title() for w in STOPWORDS]))\n",
    "STOPWORDS = STOPWORDS.union(set([w.translate(str.maketrans('', '', PUNCT_TO_REMOVE)) for w in STOPWORDS]))\n",
    "\n",
    "STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e045329a-1a7d-4ec8-8d3d-3dc52e361914",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df['cleaned_review'] = reviews_df['cleaned_review'].apply(lambda x: \" \".join([word for word in x.split() if word not in STOPWORDS]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6c96894-ddcc-4616-ba1c-2bf5e4a41e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     caught little gem totally accident back reviva...\n",
       "1     cant believe let movie accomplish favor friend...\n",
       "2     spoiler alert gets nerve people remake use ter...\n",
       "3     theres one thing Ive learnt watching George Ro...\n",
       "4     remember theaters reviews said horrible Well t...\n",
       "5     Opera US title terror opera somewhat letdown D...\n",
       "6     Heard film long ago finally found ebay five bu...\n",
       "8     worth mentioning omitted reviews read subtext ...\n",
       "9     Darling Lili fantastic far one favorite films ...\n",
       "10    Twentieth CenturyFox made ton Mr Moto films Ho...\n",
       "Name: cleaned_review, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df['cleaned_review'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7e95be-ffa6-4c44-b4a1-0f79db61ea90",
   "metadata": {},
   "source": [
    "## Tokenization.\n",
    "\n",
    "We are using standard NLTK words tokenization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4156a2e9-2e44-42d7-932f-869df9e3d77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994fb2674ce64bc5998f9e3d7b98b4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=9193), Label(value='0 / 9193'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_words(\n",
    "    text: str\n",
    "):\n",
    "    import nltk\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "    return nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "reviews_df['tokenized_review'] = reviews_df['cleaned_review'].parallel_apply(tokenize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87c1d0d-0140-4202-b3e7-44402429a906",
   "metadata": {},
   "source": [
    "## Lemmatization.\n",
    "\n",
    "In computational linguistics, lemmatization is the algorithmic process of determining the lemma of a word based on its intended meaning. Unlike stemming, lemmatization depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighbouring sentences or even an entire document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dd61b76-e5b2-4526-b594-19c7a6977977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(\n",
    "    text: list\n",
    "):\n",
    "    import nltk\n",
    "    from nltk.corpus import wordnet\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('averaged_perceptron_tagger_eng')\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordnet_map = {\n",
    "        \"N\": wordnet.NOUN, \n",
    "        \"V\": wordnet.VERB, \n",
    "        \"J\": wordnet.ADJ, \n",
    "        \"R\": wordnet.ADV\n",
    "    }\n",
    "    pos_tagged_text = nltk.pos_tag(text)\n",
    "    return [lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "641e3b67-cd72-427d-aef0-56588596d71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31e8eb983e241a88639ccb78db73622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=9193), Label(value='0 / 9193'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reviews_df['lemmatized_review'] = reviews_df['tokenized_review'].parallel_apply(lemmatize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0468905-e160-407b-9807-b4101d4999d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [catch, little, gem, totally, accident, back, ...\n",
       "1    [cant, believe, let, movie, accomplish, favor,...\n",
       "2    [spoiler, alert, get, nerve, people, remake, u...\n",
       "3    [there, one, thing, Ive, learnt, watch, George...\n",
       "4    [remember, theater, review, say, horrible, Wel...\n",
       "Name: lemmatized_review, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df['lemmatized_review'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef7b306-41fb-4ed5-a44c-5e5f0f830f2d",
   "metadata": {},
   "source": [
    "## Stemming.\n",
    "\n",
    "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b5d00bf-3ba3-4af5-8df8-2240d10396b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(\n",
    "    text\n",
    "):\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    return [stemmer.stem(word) for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4c77ac3-bcd0-4d5d-9685-c80ab4f17b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f357c13b8ce4d4cbca4b996279f5fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=9193), Label(value='0 / 9193'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reviews_df['stemmed_review'] = reviews_df['tokenized_review'].parallel_apply(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5675b90c-284c-46a3-a10f-4dfe0a22173a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [caught, littl, gem, total, accid, back, reviv...\n",
       "1    [cant, believ, let, movi, accomplish, favor, f...\n",
       "2    [spoiler, alert, get, nerv, peopl, remak, use,...\n",
       "3    [there, one, thing, ive, learnt, watch, georg,...\n",
       "4    [rememb, theater, review, said, horribl, well,...\n",
       "Name: stemmed_review, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df['stemmed_review'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec41651f-e80b-4285-9dca-70af205c16ef",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization comparison.\n",
    "\n",
    "In the following section we will analyze results from both stemming and lemmatization, based on the following criterias:\n",
    "1. Number of unique tokens produced by stemming and lemmatization.\n",
    "2. Difference between number of shorted tokens (with length < 2) produced by two approaches.\n",
    "3. Modifications of initial tokens after operation preforming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef608e1a-43ba-4d07-85cf-721a0d288df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_words(\n",
    "    df: pd.DataFrame,\n",
    "    col_name: str\n",
    "): \n",
    "    cntr = Counter()\n",
    "    for processed_tokens in df[col_name].values:\n",
    "        for token in processed_tokens:\n",
    "            cntr[token] += 1\n",
    "    return cntr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09f76ad9-3556-469a-8d58-e15c74906186",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemm_counter = count_words(reviews_df, 'stemmed_review')\n",
    "lemm_counter = count_words(reviews_df, 'lemmatized_review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4543812e-018a-442b-8a26-2eddcfc2adcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique stemmed words: 93786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('movi', 70630),\n",
       " ('film', 59314),\n",
       " ('one', 33908),\n",
       " ('like', 28273),\n",
       " ('good', 19876),\n",
       " ('time', 19668),\n",
       " ('watch', 19329),\n",
       " ('see', 18310),\n",
       " ('make', 18160),\n",
       " ('get', 17379)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of unique stemmed words:\", len(stemm_counter))\n",
    "stemm_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b17a58b-5bda-4f30-844b-39b88a394f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique lemmatized words: 138953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('movie', 69502),\n",
       " ('film', 58016),\n",
       " ('one', 30981),\n",
       " ('make', 27532),\n",
       " ('see', 26538),\n",
       " ('like', 26363),\n",
       " ('get', 21789),\n",
       " ('good', 21553),\n",
       " ('time', 19026),\n",
       " ('watch', 17412)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of unique lemmatized words:\", len(lemm_counter))\n",
    "lemm_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f90c980e-e0eb-4211-b326-aa74d2f33e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_number_of_short_words(\n",
    "    cntr\n",
    "):\n",
    "    data = []\n",
    "    for k, v in cntr.items():\n",
    "        if len(k) < 3:\n",
    "            data.append((v, k))\n",
    "    data.sort(reverse=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0135d2b3-f03f-412b-9b0a-ea8a195e6363",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemm_short_words = calculate_number_of_short_words(stemm_counter)\n",
    "lemm_short_words = calculate_number_of_short_words(lemm_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca79f848-9ca6-4768-a217-6b5d4bc686fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stemmed words with length less than two: 534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(11162, 'go'),\n",
       " (6299, 'im'),\n",
       " (4487, 'us'),\n",
       " (3739, 'tv'),\n",
       " (3235, 'he'),\n",
       " (2411, 'aw'),\n",
       " (1979, 'mr'),\n",
       " (1804, 'ye'),\n",
       " (1794, 'id'),\n",
       " (1746, 'oh'),\n",
       " (1372, 'ok'),\n",
       " (983, 'ad'),\n",
       " (915, 'th'),\n",
       " (857, 'dr'),\n",
       " (813, 'b'),\n",
       " (740, 'la'),\n",
       " (737, 'de'),\n",
       " (537, 'na'),\n",
       " (513, 'of'),\n",
       " (505, 'ed')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of stemmed words with length less than two:\", len(stemm_short_words))\n",
    "stemm_short_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f0ecf77-4c15-4725-bd32-d83499edb4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lemmatized words with length less than two: 931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(16095, 'go'),\n",
       " (5953, 'Im'),\n",
       " (3878, 'do'),\n",
       " (3816, 'u'),\n",
       " (3494, 'TV'),\n",
       " (2174, 'he'),\n",
       " (1671, 'Id'),\n",
       " (1631, 'Mr'),\n",
       " (1200, 'Oh'),\n",
       " (1166, 'OK'),\n",
       " (895, 'th'),\n",
       " (851, 'US'),\n",
       " (824, 'Dr'),\n",
       " (678, 'B'),\n",
       " (522, 'na'),\n",
       " (492, 'oh'),\n",
       " (488, 'OF'),\n",
       " (464, 'Ed'),\n",
       " (427, 'Ms'),\n",
       " (424, 'II')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of lemmatized words with length less than two:\", len(lemm_short_words))\n",
    "lemm_short_words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abfe052-5d07-413e-bd6e-ccb644a7402e",
   "metadata": {},
   "source": [
    "# Vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7822485-4311-4148-bd61-e5970898c11d",
   "metadata": {},
   "source": [
    "## Count Vectorizer.\n",
    "\n",
    "The count vectorizer is a customizable SciKit Learn preprocessor method. It works with any text out of the box, and applies preprocessing, tokenization and stop words removal on its own. These tasks can be customized, for example by providing a different tokenization method or stop word list. (This applies to all other preprocessors as well.) Applying the count vectorizer to raw text creates a matrix in the form of (document_id, tokens) in which the values are the token count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0bed256-7f57-4cdb-a2eb-822e429a0637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_x_train = count_vectorizer.fit_transform(reviews_df['lemmatized_review'].apply(lambda x: \" \".join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49af3835-acb7-47a2-8b44-0e42a6fbc48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_x_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ce11bc5-319d-4f4f-9773-80b2e0242284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aa', 'aaa', 'aaaaaaaargh', ..., 'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz',\n",
       "       'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz',\n",
       "       'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz'], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd76704-52ff-4c05-b363-c8f51beabbbb",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer.\n",
    "\n",
    "The Term Frequency/Inverse Document Frequency is a well-known metric in information retrieval. It encodes word frequencies in such a way as to put equal weight to common terms that occur in many documents, as well as uncommon terms only present in a few documents. This metric generalizes well over large corpora and improves finding relevant topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d542cd7a-a47a-494a-9ca5-ba91356b03ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_x_train = tfidf_vectorizer.fit_transform(reviews_df['lemmatized_review'].apply(lambda x: \" \".join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8452e49a-ab99-460d-93f3-783c5b3bd421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aa', 'aaa', 'aaaaaaaargh', ..., 'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz',\n",
       "       'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz',\n",
       "       'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz'], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9040f345-1a54-4e0c-b6eb-57f071282d8c",
   "metadata": {},
   "source": [
    "## TF-IDF and Count Vectorizers comparison.\n",
    "\n",
    "We will look through the following aspects:\n",
    "1. Number of features recognized by each approach.\n",
    "2. Shapes of processed datasets.\n",
    "3. Time required for each approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4d0f434-4c2a-435b-a0a0-924583999541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36770, 117172)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a5261ed-d4e4-4503-abe9-65408b9af215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36770, 117172)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1097a1b3-f526-456d-9d31-2644c720e2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117172, 117172)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_vectorizer.get_feature_names_out()), len(count_vectorizer.get_feature_names_out())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
